{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb27a01-0d7d-4ffc-ae36-8665b7468353",
   "metadata": {},
   "source": [
    "# Supervised baseline: TubeSpam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccfd3f4-893a-4386-8b95-0f3c822da406",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c0dc4c-ae9f-4e2c-8b70-0d1b84e30012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier # Linear SVM.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2383b-33eb-42a1-9eff-339e12ee6bc7",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e19087-22a0-497b-acd8-b583add73aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3858 entries, 0 to 3857\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   person1_word_idx      3858 non-null   object\n",
      " 1   person2_word_idx      3858 non-null   object\n",
      " 2   sentence              3858 non-null   object\n",
      " 3   tokens                3858 non-null   object\n",
      " 4   person1_right_tokens  3858 non-null   object\n",
      " 5   person2_right_tokens  3858 non-null   object\n",
      " 6   between_tokens        3858 non-null   object\n",
      " 7   Label                 3858 non-null   int64 \n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 241.2+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person1_word_idx</th>\n",
       "      <th>person2_word_idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokens</th>\n",
       "      <th>person1_right_tokens</th>\n",
       "      <th>person2_right_tokens</th>\n",
       "      <th>between_tokens</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>(7, 8)</td>\n",
       "      <td>With Dellen Millard, 30, and Mark Smich, 27, i...</td>\n",
       "      <td>['With', 'Dellen', 'Millard', ',', '30', ',', ...</td>\n",
       "      <td>[',', '30', ',', 'and']</td>\n",
       "      <td>[',', '27', ',', 'in']</td>\n",
       "      <td>[',', '30', ',', 'and']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>John is in Ukraine, where he met President Pet...</td>\n",
       "      <td>['John', 'is', 'in', 'Ukraine', ',', 'where', ...</td>\n",
       "      <td>['is', 'in', 'Ukraine', ',']</td>\n",
       "      <td>['and', 'called', 'for', 'the']</td>\n",
       "      <td>['is', 'in', 'Ukraine', ',', 'where', 'he', 'm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>(68, 68)</td>\n",
       "      <td>1 Serena Williams considers sister Venus 'the ...</td>\n",
       "      <td>['1', 'Serena', 'Williams', 'considers', 'sist...</td>\n",
       "      <td>['', 'the', 'best', 'player']</td>\n",
       "      <td>['.', '', '*']</td>\n",
       "      <td>['', 'the', 'best', 'player', 'in', 'the', 'to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>(15, 16)</td>\n",
       "      <td>Her rules: Chrissy Teigen, right, posed next t...</td>\n",
       "      <td>['Her', 'rules', ':', 'Chrissy', 'Teigen', ','...</td>\n",
       "      <td>['and', 'Ashley', 'Tisdale', ',']</td>\n",
       "      <td>[',', 'left', ',', 'at']</td>\n",
       "      <td>['and']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(44, 44)</td>\n",
       "      <td>(75, 75)</td>\n",
       "      <td>Devoted mum: Katie goes on to insist that she ...</td>\n",
       "      <td>['Devoted', 'mum', ':', 'Katie', 'goes', 'on',...</td>\n",
       "      <td>['says', 'her', 'hectic', 'schedule']</td>\n",
       "      <td>['quipped', 'back', ':', '']</td>\n",
       "      <td>['says', 'her', 'hectic', 'schedule', 'and', '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  person1_word_idx person2_word_idx  \\\n",
       "0           (1, 2)           (7, 8)   \n",
       "1           (0, 0)          (9, 10)   \n",
       "2           (5, 5)         (68, 68)   \n",
       "3         (12, 13)         (15, 16)   \n",
       "4         (44, 44)         (75, 75)   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  With Dellen Millard, 30, and Mark Smich, 27, i...   \n",
       "1  John is in Ukraine, where he met President Pet...   \n",
       "2  1 Serena Williams considers sister Venus 'the ...   \n",
       "3  Her rules: Chrissy Teigen, right, posed next t...   \n",
       "4  Devoted mum: Katie goes on to insist that she ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['With', 'Dellen', 'Millard', ',', '30', ',', ...   \n",
       "1  ['John', 'is', 'in', 'Ukraine', ',', 'where', ...   \n",
       "2  ['1', 'Serena', 'Williams', 'considers', 'sist...   \n",
       "3  ['Her', 'rules', ':', 'Chrissy', 'Teigen', ','...   \n",
       "4  ['Devoted', 'mum', ':', 'Katie', 'goes', 'on',...   \n",
       "\n",
       "                    person1_right_tokens             person2_right_tokens  \\\n",
       "0                [',', '30', ',', 'and']           [',', '27', ',', 'in']   \n",
       "1           ['is', 'in', 'Ukraine', ',']  ['and', 'called', 'for', 'the']   \n",
       "2          ['', 'the', 'best', 'player']                   ['.', '', '*']   \n",
       "3      ['and', 'Ashley', 'Tisdale', ',']         [',', 'left', ',', 'at']   \n",
       "4  ['says', 'her', 'hectic', 'schedule']     ['quipped', 'back', ':', '']   \n",
       "\n",
       "                                      between_tokens  Label  \n",
       "0                            [',', '30', ',', 'and']      0  \n",
       "1  ['is', 'in', 'Ukraine', ',', 'where', 'he', 'm...      0  \n",
       "2  ['', 'the', 'best', 'player', 'in', 'the', 'to...      0  \n",
       "3                                            ['and']      0  \n",
       "4  ['says', 'her', 'hectic', 'schedule', 'and', '...      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 553 entries, 0 to 552\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   person1_word_idx      553 non-null    object\n",
      " 1   person2_word_idx      553 non-null    object\n",
      " 2   sentence              553 non-null    object\n",
      " 3   tokens                553 non-null    object\n",
      " 4   person1_right_tokens  553 non-null    object\n",
      " 5   person2_right_tokens  553 non-null    object\n",
      " 6   between_tokens        553 non-null    object\n",
      " 7   Label                 553 non-null    int64 \n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 34.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person1_word_idx</th>\n",
       "      <th>person2_word_idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokens</th>\n",
       "      <th>person1_right_tokens</th>\n",
       "      <th>person2_right_tokens</th>\n",
       "      <th>between_tokens</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(14, 14)</td>\n",
       "      <td>Zac said of the Dawson's Creek alum: 'When I f...</td>\n",
       "      <td>['Zac', 'said', 'of', 'the', 'Dawson', 's', 'C...</td>\n",
       "      <td>['said', 'of', 'the', 'Dawson']</td>\n",
       "      <td>['and', 'her', 'rapport', 'on']</td>\n",
       "      <td>['said', 'of', 'the', 'Dawson', 's', 'Creek', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(43, 44)</td>\n",
       "      <td>(55, 56)</td>\n",
       "      <td>Fast moving couple: Morena Baccarin's estrange...</td>\n",
       "      <td>['Fast', 'moving', 'couple', ':', 'Morena', 'B...</td>\n",
       "      <td>['', 'At', 'the', 'centre']</td>\n",
       "      <td>[',', 'pictured', 'in', 'New']</td>\n",
       "      <td>['', 'At', 'the', 'centre', ':', 'Her', 'Gotha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(12, 12)</td>\n",
       "      <td>Meanwhile Marie and her partner Emi, as well a...</td>\n",
       "      <td>['Meanwhile', 'Marie', 'and', 'her', 'partner'...</td>\n",
       "      <td>['and', 'her', 'partner', 'Emi']</td>\n",
       "      <td>['and', 'Vanessa', 'from', 'Sydney']</td>\n",
       "      <td>['and', 'her', 'partner', 'Emi', ',', 'as', 'w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>Judi and Wayne Richardson (ME), parents of Dar...</td>\n",
       "      <td>['Judi', 'and', 'Wayne', 'Richardson', '(', 'M...</td>\n",
       "      <td>['and', 'Wayne', 'Richardson', '(']</td>\n",
       "      <td>[',', 'who', 'was', 'shot']</td>\n",
       "      <td>['and', 'Wayne', 'Richardson', '(', 'ME', ')',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(20, 20)</td>\n",
       "      <td>(58, 58)</td>\n",
       "      <td>Expecting: The Hotplate star Marie Yokoyama is...</td>\n",
       "      <td>['Expecting', ':', 'The', 'Hotplate', 'star', ...</td>\n",
       "      <td>[',', 'pictured', 'with', 'her']</td>\n",
       "      <td>[',', 'who', 'is', 'now']</td>\n",
       "      <td>[',', 'pictured', 'with', 'her', 'work', 'part...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  person1_word_idx person2_word_idx  \\\n",
       "0           (0, 0)         (14, 14)   \n",
       "1         (43, 44)         (55, 56)   \n",
       "2           (1, 1)         (12, 12)   \n",
       "3           (0, 0)         (10, 11)   \n",
       "4         (20, 20)         (58, 58)   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  Zac said of the Dawson's Creek alum: 'When I f...   \n",
       "1  Fast moving couple: Morena Baccarin's estrange...   \n",
       "2  Meanwhile Marie and her partner Emi, as well a...   \n",
       "3  Judi and Wayne Richardson (ME), parents of Dar...   \n",
       "4  Expecting: The Hotplate star Marie Yokoyama is...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['Zac', 'said', 'of', 'the', 'Dawson', 's', 'C...   \n",
       "1  ['Fast', 'moving', 'couple', ':', 'Morena', 'B...   \n",
       "2  ['Meanwhile', 'Marie', 'and', 'her', 'partner'...   \n",
       "3  ['Judi', 'and', 'Wayne', 'Richardson', '(', 'M...   \n",
       "4  ['Expecting', ':', 'The', 'Hotplate', 'star', ...   \n",
       "\n",
       "                  person1_right_tokens                  person2_right_tokens  \\\n",
       "0      ['said', 'of', 'the', 'Dawson']       ['and', 'her', 'rapport', 'on']   \n",
       "1          ['', 'At', 'the', 'centre']        [',', 'pictured', 'in', 'New']   \n",
       "2     ['and', 'her', 'partner', 'Emi']  ['and', 'Vanessa', 'from', 'Sydney']   \n",
       "3  ['and', 'Wayne', 'Richardson', '(']           [',', 'who', 'was', 'shot']   \n",
       "4     [',', 'pictured', 'with', 'her']             [',', 'who', 'is', 'now']   \n",
       "\n",
       "                                      between_tokens  Label  \n",
       "0  ['said', 'of', 'the', 'Dawson', 's', 'Creek', ...      0  \n",
       "1  ['', 'At', 'the', 'centre', ':', 'Her', 'Gotha...      0  \n",
       "2  ['and', 'her', 'partner', 'Emi', ',', 'as', 'w...      0  \n",
       "3  ['and', 'Wayne', 'Richardson', '(', 'ME', ')',...      0  \n",
       "4  [',', 'pictured', 'with', 'her', 'work', 'part...      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1101 entries, 0 to 1100\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   person1_word_idx      1101 non-null   object\n",
      " 1   person2_word_idx      1101 non-null   object\n",
      " 2   sentence              1101 non-null   object\n",
      " 3   tokens                1101 non-null   object\n",
      " 4   person1_right_tokens  1101 non-null   object\n",
      " 5   person2_right_tokens  1101 non-null   object\n",
      " 6   between_tokens        1101 non-null   object\n",
      " 7   Label                 1101 non-null   int64 \n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 68.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person1_word_idx</th>\n",
       "      <th>person2_word_idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokens</th>\n",
       "      <th>person1_right_tokens</th>\n",
       "      <th>person2_right_tokens</th>\n",
       "      <th>between_tokens</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>Coordinated: Sam Burgess and fiancé Phoebe Hoo...</td>\n",
       "      <td>['Coordinated', ':', 'Sam', 'Burgess', 'and', ...</td>\n",
       "      <td>['and', 'fiancé', 'Phoebe', 'Hooke']</td>\n",
       "      <td>[',', 'who', 'have', 'now']</td>\n",
       "      <td>['and', 'fiancé']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>After Betty's death, Robert filed forms to the...</td>\n",
       "      <td>['After', 'Betty', 's', 'death', ',', 'Robert'...</td>\n",
       "      <td>['s', 'death', ',', 'Robert']</td>\n",
       "      <td>['filed', 'forms', 'to', 'the']</td>\n",
       "      <td>['s', 'death', ',']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(23, 24)</td>\n",
       "      <td>(43, 47)</td>\n",
       "      <td>Ceawlin Thynn, 41, said the row resulted in hi...</td>\n",
       "      <td>['Ceawlin', 'Thynn', ',', '41', ',', 'said', '...</td>\n",
       "      <td>[',', '29', '(', 'pictured']</td>\n",
       "      <td>['pictured', 'on', 'the', 'grounds']</td>\n",
       "      <td>[',', '29', '(', 'pictured', 'above', ')', ','...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(9, 11)</td>\n",
       "      <td>(24, 24)</td>\n",
       "      <td>Source: News unlimited - 1 day ago  Sheena Bor...</td>\n",
       "      <td>['Source', ':', 'News', 'unlimited', '-', '1',...</td>\n",
       "      <td>['case', ':', '14', 'days']</td>\n",
       "      <td>[',', 'Sept', '07', '(']</td>\n",
       "      <td>['case', ':', '14', 'days', 'judicial', 'custo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>(6, 6)</td>\n",
       "      <td>'      Loved-up: Sofía and Joe Manganiello are...</td>\n",
       "      <td>['', '', 'Loved', '-', 'up', ':', 'Sofía', 'an...</td>\n",
       "      <td>[':', 'Sofía', 'and', 'Joe']</td>\n",
       "      <td>['and', 'Joe', 'Manganiello', '']</td>\n",
       "      <td>[':']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  person1_word_idx person2_word_idx  \\\n",
       "0           (2, 3)           (6, 7)   \n",
       "1           (1, 1)           (5, 5)   \n",
       "2         (23, 24)         (43, 47)   \n",
       "3          (9, 11)         (24, 24)   \n",
       "4           (1, 4)           (6, 6)   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  Coordinated: Sam Burgess and fiancé Phoebe Hoo...   \n",
       "1  After Betty's death, Robert filed forms to the...   \n",
       "2  Ceawlin Thynn, 41, said the row resulted in hi...   \n",
       "3  Source: News unlimited - 1 day ago  Sheena Bor...   \n",
       "4  '      Loved-up: Sofía and Joe Manganiello are...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['Coordinated', ':', 'Sam', 'Burgess', 'and', ...   \n",
       "1  ['After', 'Betty', 's', 'death', ',', 'Robert'...   \n",
       "2  ['Ceawlin', 'Thynn', ',', '41', ',', 'said', '...   \n",
       "3  ['Source', ':', 'News', 'unlimited', '-', '1',...   \n",
       "4  ['', '', 'Loved', '-', 'up', ':', 'Sofía', 'an...   \n",
       "\n",
       "                   person1_right_tokens                  person2_right_tokens  \\\n",
       "0  ['and', 'fiancé', 'Phoebe', 'Hooke']           [',', 'who', 'have', 'now']   \n",
       "1         ['s', 'death', ',', 'Robert']       ['filed', 'forms', 'to', 'the']   \n",
       "2          [',', '29', '(', 'pictured']  ['pictured', 'on', 'the', 'grounds']   \n",
       "3           ['case', ':', '14', 'days']              [',', 'Sept', '07', '(']   \n",
       "4          [':', 'Sofía', 'and', 'Joe']     ['and', 'Joe', 'Manganiello', '']   \n",
       "\n",
       "                                      between_tokens  Label  \n",
       "0                                  ['and', 'fiancé']      1  \n",
       "1                                ['s', 'death', ',']      0  \n",
       "2  [',', '29', '(', 'pictured', 'above', ')', ','...      0  \n",
       "3  ['case', ':', '14', 'days', 'judicial', 'custo...      0  \n",
       "4                                              [':']      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read data files.\n",
    "df_train = pd.read_csv(\"../../data/Spouse/spouse_train.csv\")\n",
    "df_val = pd.read_csv(\"../../data/Spouse/spouse_val.csv\")\n",
    "df_test = pd.read_csv(\"../../data/Spouse/spouse_test.csv\")\n",
    "\n",
    "# Explore data.\n",
    "print(df_train.info())\n",
    "display(df_train.head())\n",
    "\n",
    "print(df_val.info())\n",
    "display(df_val.head())\n",
    "\n",
    "print(df_test.info())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b0f43-1abb-43d7-a914-6b86b0d38e50",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c27fd9-692c-4b39-916c-e8f65ba3ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3858, 10979)\n",
      "  (0, 10790)\t1\n",
      "  (0, 2834)\t1\n",
      "  (0, 6490)\t1\n",
      "  (0, 154)\t1\n",
      "  (0, 646)\t1\n",
      "  (0, 6229)\t1\n",
      "  (0, 9164)\t1\n",
      "  (0, 143)\t1\n",
      "  (0, 5053)\t2\n",
      "  (0, 9910)\t3\n",
      "  (0, 7817)\t1\n",
      "  (0, 1463)\t1\n",
      "  (0, 3982)\t1\n",
      "  (0, 7753)\t1\n",
      "  (0, 7836)\t1\n",
      "  (0, 482)\t1\n",
      "  (0, 7033)\t1\n",
      "  (0, 9918)\t1\n",
      "  (0, 3891)\t1\n",
      "  (0, 2810)\t1\n",
      "  (0, 6691)\t1\n",
      "  (0, 10169)\t1\n",
      "  (0, 5492)\t1\n",
      "  (0, 5322)\t1\n",
      "  (0, 10246)\t1\n",
      "  :\t:\n",
      "  (3857, 279)\t1\n",
      "  (3857, 4802)\t1\n",
      "  (3857, 9938)\t1\n",
      "  (3857, 4647)\t1\n",
      "  (3857, 10043)\t1\n",
      "  (3857, 6665)\t2\n",
      "  (3857, 9948)\t1\n",
      "  (3857, 10673)\t1\n",
      "  (3857, 3968)\t1\n",
      "  (3857, 6207)\t1\n",
      "  (3857, 3813)\t1\n",
      "  (3857, 6246)\t1\n",
      "  (3857, 6668)\t1\n",
      "  (3857, 7750)\t1\n",
      "  (3857, 117)\t1\n",
      "  (3857, 9923)\t1\n",
      "  (3857, 7587)\t1\n",
      "  (3857, 7372)\t2\n",
      "  (3857, 5453)\t1\n",
      "  (3857, 2718)\t2\n",
      "  (3857, 3234)\t1\n",
      "  (3857, 3238)\t1\n",
      "  (3857, 7160)\t1\n",
      "  (3857, 9873)\t1\n",
      "  (3857, 6468)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3858, 10979)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build dictionary of features and transform documents to feature vectors.\n",
    "# Value of a word in the vocabulary is its frequency in the whole training corpus.\n",
    "# https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(df_train[\"sentence\"])\n",
    "print(X_train_counts.shape)\n",
    "print(X_train_counts)\n",
    "\n",
    "# Regularize via “Term Frequency times Inverse Document Frequency.”\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2b8ba4-51a6-4032-b57f-38c922e7c3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1101, 10979)\n",
      "  (0, 10828)\t0.1026099438506754\n",
      "  (0, 10726)\t0.05070541563899101\n",
      "  (0, 10722)\t0.10135966630424763\n",
      "  (0, 10063)\t0.09265420721337225\n",
      "  (0, 10054)\t0.13545109451982273\n",
      "  (0, 10034)\t0.1240751959076648\n",
      "  (0, 9910)\t0.14892683727632133\n",
      "  (0, 9618)\t0.11647145069527888\n",
      "  (0, 9565)\t0.1574210630720089\n",
      "  (0, 9476)\t0.15295865458125757\n",
      "  (0, 9397)\t0.07326144597667783\n",
      "  (0, 9346)\t0.1493126013644262\n",
      "  (0, 9332)\t0.11293320327690753\n",
      "  (0, 8997)\t0.14929294245214494\n",
      "  (0, 8992)\t0.1435595562274054\n",
      "  (0, 8641)\t0.1323684004492796\n",
      "  (0, 8528)\t0.14120413965684353\n",
      "  (0, 8181)\t0.3150171585632369\n",
      "  (0, 7538)\t0.09606636387867862\n",
      "  (0, 7516)\t0.1574210630720089\n",
      "  (0, 7035)\t0.0799011815717665\n",
      "  (0, 7033)\t0.0330519940442177\n",
      "  (0, 6961)\t0.08279480917255869\n",
      "  (0, 6652)\t0.11850689360467614\n",
      "  (0, 6286)\t0.1323684004492796\n",
      "  :\t:\n",
      "  (1099, 1307)\t0.1635262627846521\n",
      "  (1099, 1040)\t0.14761029778948762\n",
      "  (1099, 879)\t0.04895063552587649\n",
      "  (1099, 826)\t0.09433125275646147\n",
      "  (1099, 803)\t0.08861197890849595\n",
      "  (1099, 646)\t0.05071762150790654\n",
      "  (1099, 539)\t0.1286411416959279\n",
      "  (1100, 9975)\t0.2051234807711815\n",
      "  (1100, 9593)\t0.21380691880049896\n",
      "  (1100, 9413)\t0.232718912127374\n",
      "  (1100, 9049)\t0.2804453843298965\n",
      "  (1100, 8893)\t0.3166939981008933\n",
      "  (1100, 8261)\t0.25005553257149576\n",
      "  (1100, 6665)\t0.1553852760122934\n",
      "  (1100, 6427)\t0.42624505249932865\n",
      "  (1100, 5790)\t0.21973198155843693\n",
      "  (1100, 5053)\t0.06579783038665914\n",
      "  (1100, 4802)\t0.09236786512655751\n",
      "  (1100, 4528)\t0.12593509018923948\n",
      "  (1100, 2238)\t0.29235680490747573\n",
      "  (1100, 2088)\t0.24250654376999234\n",
      "  (1100, 2082)\t0.20456770321008821\n",
      "  (1100, 1810)\t0.25005553257149576\n",
      "  (1100, 1372)\t0.2553484485982996\n",
      "  (1100, 1169)\t0.1410231516608089\n"
     ]
    }
   ],
   "source": [
    "# Call transform only, as featurizers have already been fit to training data.\n",
    "X_test_counts = count_vect.transform(df_test[\"sentence\"])\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "print(X_test_tfidf.shape)\n",
    "print(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7cb6ae-5a00-4781-9d46-2c0920ffe807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3858\n",
      "0    0.925868\n",
      "1    0.074132\n",
      "Name: Label, dtype: float64\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: Label, dtype: int64\n",
      "1101\n",
      "0    0.926431\n",
      "1    0.073569\n",
      "Name: Label, dtype: float64\n",
      "0    1\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract labels.\n",
    "y_train = df_train[\"Label\"]\n",
    "y_test = df_test[\"Label\"]\n",
    "\n",
    "print(len(y_train))\n",
    "print(y_train.value_counts(normalize = True))\n",
    "print(y_train.head())\n",
    "\n",
    "print(len(y_test))\n",
    "print(y_test.value_counts(normalize = True))\n",
    "print(y_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b974f10-bbd9-4d73-8de7-8d351cd04613",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2c37bf-5383-4fcc-a591-7a1594e57c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a multinomial Naive Bayes model.\n",
    "mnb = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ac471c-c794-4d93-a73b-2b9854c7cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM.\n",
    "# Regularized linear models with stochastic gradient descent (SGD) learning.\n",
    "svm = SGDClassifier(loss = \"hinge\", \n",
    "                    penalty = \"l2\",\n",
    "                    alpha = 1e-3, \n",
    "                    random_state = 42,\n",
    "                    max_iter = 5, \n",
    "                    tol = None)\n",
    "svm = svm.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e9e6a4-125e-4dd1-ad0a-4cdbccb11ee8",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3ac8fa1-3714-4715-b4ee-92fb0d779c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NAIVE BAYES ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1020\n",
      "           1       0.00      0.00      0.00        81\n",
      "\n",
      "    accuracy                           0.93      1101\n",
      "   macro avg       0.46      0.50      0.48      1101\n",
      "weighted avg       0.86      0.93      0.89      1101\n",
      "\n",
      "\n",
      "--- NAIVE BAYES ---\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "tn, fp, fn, tp = [1020    0   81    0]\n",
      "F1             = 0.0\n",
      "Accuracy       = 0.9264305177111717\n",
      "Precision      = 0.0\n",
      "Recall         = 0.0\n",
      "ROC AUC        = 0.5\n",
      "---------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set.\n",
    "y_mnb = mnb.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate performance.\n",
    "print(\"\\n--- NAIVE BAYES ---\\n\")\n",
    "print(metrics.classification_report(y_test, y_mnb))\n",
    "\n",
    "# MNB performance metrics.\n",
    "# 0 = HAM, 1 = SPAM.\n",
    "# Care most about false negatives – keeping SPAM when we only want HAM.\n",
    "# Recall, F1, and accuracy rely on false negatives.\n",
    "confusion_mnb = metrics.confusion_matrix(y_test, y_mnb)\n",
    "acc_mnb = metrics.accuracy_score(y_test, y_mnb)\n",
    "f1_mnb = metrics.f1_score(y_test, y_mnb, zero_division = 0)\n",
    "precision_mnb = metrics.precision_score(y_test, y_mnb, zero_division = 0)\n",
    "recall_mnb = metrics.recall_score(y_test, y_mnb, zero_division = 0)\n",
    "roc_mnb = metrics.roc_auc_score(y_test, y_mnb)\n",
    "        \n",
    "print(\"\\n--- NAIVE BAYES ---\\n\")\n",
    "print(\"\\n---------------------------------------------\")\n",
    "print(\"tn, fp, fn, tp =\", confusion_mnb.ravel())\n",
    "print(\"F1             =\", f1_mnb)\n",
    "print(\"Accuracy       =\", acc_mnb)\n",
    "print(\"Precision      =\", precision_mnb)\n",
    "print(\"Recall         =\", recall_mnb)\n",
    "print(\"ROC AUC        =\", roc_mnb)\n",
    "print(\"---------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0a972f0-0b2c-4f93-9c77-45983b11d066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LINEAR SUPPORT VECTOR MACHINE ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1020\n",
      "           1       0.00      0.00      0.00        81\n",
      "\n",
      "    accuracy                           0.93      1101\n",
      "   macro avg       0.46      0.50      0.48      1101\n",
      "weighted avg       0.86      0.93      0.89      1101\n",
      "\n",
      "\n",
      "--- LINEAR SUPPORT VECTOR MACHINE ---\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "tn, fp, fn, tp = [1020    0   81    0]\n",
      "F1             = 0.0\n",
      "Accuracy       = 0.9264305177111717\n",
      "Precision      = 0.0\n",
      "Recall         = 0.0\n",
      "ROC AUC        = 0.5\n",
      "---------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set.\n",
    "y_svm = svm.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate performance.\n",
    "print(\"\\n--- LINEAR SUPPORT VECTOR MACHINE ---\\n\")\n",
    "print(metrics.classification_report(y_test, y_svm))\n",
    "\n",
    "# SVM performance metrics.\n",
    "# 0 = HAM, 1 = SPAM.\n",
    "# Care most about false negatives – keeping SPAM when we only want HAM.\n",
    "# Recall, F1, and accuracy rely on false negatives.\n",
    "confusion_svm = metrics.confusion_matrix(y_test, y_svm)\n",
    "acc_svm = metrics.accuracy_score(y_test, y_svm)\n",
    "f1_svm = metrics.f1_score(y_test, y_svm, zero_division = 0)\n",
    "precision_svm = metrics.precision_score(y_test, y_svm, zero_division = 0)\n",
    "recall_svm = metrics.recall_score(y_test, y_svm, zero_division = 0)\n",
    "roc_svm = metrics.roc_auc_score(y_test, y_svm)\n",
    "        \n",
    "print(\"\\n--- LINEAR SUPPORT VECTOR MACHINE ---\\n\")\n",
    "print(\"\\n---------------------------------------------\")\n",
    "print(\"tn, fp, fn, tp =\", confusion_svm.ravel())\n",
    "print(\"F1             =\", f1_svm)\n",
    "print(\"Accuracy       =\", acc_svm)\n",
    "print(\"Precision      =\", precision_svm)\n",
    "print(\"Recall         =\", recall_svm)\n",
    "print(\"ROC AUC        =\", roc_svm)\n",
    "print(\"---------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ee1a8-c6f7-4ed6-8d28-14fd9ffcfc4f",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3f555dc-96a8-4422-bae3-636e949ffdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TUNED NAIVE BAYES ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93      1020\n",
      "           1       0.19      0.20      0.19        81\n",
      "\n",
      "    accuracy                           0.88      1101\n",
      "   macro avg       0.56      0.57      0.56      1101\n",
      "weighted avg       0.88      0.88      0.88      1101\n",
      "\n",
      "\n",
      "--- TUNED NAIVE BAYES ---\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "tn, fp, fn, tp = [952  68  65  16]\n",
      "F1             = 0.1939393939393939\n",
      "Accuracy       = 0.8792007266121707\n",
      "Precision      = 0.19047619047619047\n",
      "Recall         = 0.19753086419753085\n",
      "ROC AUC        = 0.5654320987654321\n",
      "---------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Init search space for SVM.\n",
    "parameters = {'alpha': [0.0, 0.01, 0.1, 0.5, 1.0, 10.0]}\n",
    "\n",
    "# Tune SVM with 5-fold cross-validation.\n",
    "mnb_tuned = GridSearchCV(MultinomialNB(), \n",
    "                         parameters,\n",
    "                         scoring = \"f1\",\n",
    "                         cv = 5, \n",
    "                         n_jobs = -1)\n",
    "\n",
    "# Fit tuned model.\n",
    "mnb_tuned = mnb_tuned.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on test set.\n",
    "y_mnb_tuned = mnb_tuned.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate performance.\n",
    "print(\"\\n--- TUNED NAIVE BAYES ---\\n\")\n",
    "print(metrics.classification_report(y_test, y_mnb_tuned))\n",
    "\n",
    "# MNB tuned performance metrics.\n",
    "# 0 = HAM, 1 = SPAM.\n",
    "# Care most about false negatives – keeping SPAM when we only want HAM.\n",
    "# Recall, F1, and accuracy rely on false negatives.\n",
    "confusion_mnb_tuned = metrics.confusion_matrix(y_test, y_mnb_tuned)\n",
    "acc_mnb_tuned = metrics.accuracy_score(y_test, y_mnb_tuned)\n",
    "f1_mnb_tuned = metrics.f1_score(y_test, y_mnb_tuned, zero_division = 0)\n",
    "precision_mnb_tuned = metrics.precision_score(y_test, y_mnb_tuned, zero_division = 0)\n",
    "recall_mnb_tuned = metrics.recall_score(y_test, y_mnb_tuned, zero_division = 0)\n",
    "roc_mnb_tuned = metrics.roc_auc_score(y_test, y_mnb_tuned)\n",
    "        \n",
    "print(\"\\n--- TUNED NAIVE BAYES ---\\n\")\n",
    "print(\"\\n---------------------------------------------\")\n",
    "print(\"tn, fp, fn, tp =\", confusion_mnb_tuned.ravel())\n",
    "print(\"F1             =\", f1_mnb_tuned)\n",
    "print(\"Accuracy       =\", acc_mnb_tuned)\n",
    "print(\"Precision      =\", precision_mnb_tuned)\n",
    "print(\"Recall         =\", recall_mnb_tuned)\n",
    "print(\"ROC AUC        =\", roc_mnb_tuned)\n",
    "print(\"---------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc74e76-42c2-4b13-86e7-5d7dbea8a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "'''\n",
    "# Init search space for SVM.\n",
    "parameters = {'clf__alpha': (1e-1, 1e-2, 1e-3)}\n",
    "\n",
    "# Tune SVM with 5-fold cross-validation.\n",
    "svm_tuned = SGDClassifier(loss = \"hinge\", \n",
    "                          penalty = \"l2\",\n",
    "                          random_state = 42,\n",
    "                          max_iter = 5, \n",
    "                          tol = None)\n",
    "svm_tuned = GridSearchCV(svm_tuned, \n",
    "                         parameters,\n",
    "                         cv = 5, \n",
    "                         scoring = \"f1\",\n",
    "                         n_jobs = -1)\n",
    "\n",
    "# Fit tuned model.\n",
    "svm_tuned = svm_tuned.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on test set.\n",
    "y_svm_tuned = svm_tuned.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate performance.\n",
    "print(\"\\n--- TUNED LINEAR SUPPORT VECTOR MACHINE ---\\n\")\n",
    "print(metrics.classification_report(y_test, y_svm_tuned))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "becaafd9-f1dd-4fbd-b417-7776e4ff6bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LINEAR SUPPORT VECTOR MACHINE ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1020\n",
      "           1       0.00      0.00      0.00        81\n",
      "\n",
      "    accuracy                           0.93      1101\n",
      "   macro avg       0.46      0.50      0.48      1101\n",
      "weighted avg       0.86      0.93      0.89      1101\n",
      "\n",
      "\n",
      "--- LINEAR SUPPORT VECTOR MACHINE ---\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "tn, fp, fn, tp = [1020    0   81    0]\n",
      "F1             = 0.0\n",
      "Accuracy       = 0.9264305177111717\n",
      "Precision      = 0.0\n",
      "Recall         = 0.0\n",
      "ROC AUC        = 0.5\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "--- CLASSIFICATION REPORT ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1020\n",
      "           1       0.00      0.00      0.00        81\n",
      "\n",
      "    accuracy                           0.93      1101\n",
      "   macro avg       0.46      0.50      0.48      1101\n",
      "weighted avg       0.86      0.93      0.89      1101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train a linear SVM.\n",
    "# Regularized linear models with stochastic gradient descent (SGD) learning.\n",
    "svm_tuned = SGDClassifier(loss = \"hinge\", \n",
    "                    penalty = \"l2\",\n",
    "                    alpha = 1e-2, \n",
    "                    random_state = 42,\n",
    "                    max_iter = 5, \n",
    "                    tol = None)\n",
    "svm_tuned = svm_tuned.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on test set.\n",
    "y_svm_tuned = svm_tuned.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate performance.\n",
    "print(\"\\n--- LINEAR SUPPORT VECTOR MACHINE ---\\n\")\n",
    "print(metrics.classification_report(y_test, y_svm_tuned))\n",
    "\n",
    "# SVM performance metrics.\n",
    "# 0 = HAM, 1 = SPAM.\n",
    "# Care most about false negatives – keeping SPAM when we only want HAM.\n",
    "# Recall, F1, and accuracy rely on false negatives.\n",
    "confusion_svm_tuned = metrics.confusion_matrix(y_test, y_svm_tuned)\n",
    "acc_svm_tuned = metrics.accuracy_score(y_test, y_svm_tuned)\n",
    "f1_svm_tuned = metrics.f1_score(y_test, y_svm_tuned, zero_division = 0)\n",
    "precision_svm_tuned = metrics.precision_score(y_test, y_svm_tuned, zero_division = 0)\n",
    "recall_svm_tuned = metrics.recall_score(y_test, y_svm_tuned, zero_division = 0)\n",
    "roc_svm_tuned = metrics.roc_auc_score(y_test, y_svm_tuned)\n",
    "        \n",
    "print(\"\\n--- LINEAR SUPPORT VECTOR MACHINE ---\\n\")\n",
    "print(\"\\n---------------------------------------------\")\n",
    "print(\"tn, fp, fn, tp =\", confusion_svm_tuned.ravel())\n",
    "print(\"F1             =\", f1_svm_tuned)\n",
    "print(\"Accuracy       =\", acc_svm_tuned)\n",
    "print(\"Precision      =\", precision_svm_tuned)\n",
    "print(\"Recall         =\", recall_svm_tuned)\n",
    "print(\"ROC AUC        =\", roc_svm_tuned)\n",
    "print(\"---------------------------------------------\\n\")\n",
    "\n",
    "# Evaluate performance.\n",
    "print(\"\\n--- CLASSIFICATION REPORT ---\\n\")\n",
    "print(metrics.classification_report(y_test, y_svm_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dfe905d-0c53-44af-9653-8e5a732ed538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LINEAR SUPPORT VECTOR MACHINE ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1020\n",
      "           1       0.00      0.00      0.00        81\n",
      "\n",
      "    accuracy                           0.93      1101\n",
      "   macro avg       0.46      0.50      0.48      1101\n",
      "weighted avg       0.86      0.93      0.89      1101\n",
      "\n",
      "\n",
      "--- LINEAR SUPPORT VECTOR MACHINE ---\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "tn, fp, fn, tp = [1020    0   81    0]\n",
      "F1             = 0.0\n",
      "Accuracy       = 0.9264305177111717\n",
      "Precision      = 0.0\n",
      "Recall         = 0.0\n",
      "ROC AUC        = 0.5\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "--- CLASSIFICATION REPORT ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1020\n",
      "           1       0.00      0.00      0.00        81\n",
      "\n",
      "    accuracy                           0.93      1101\n",
      "   macro avg       0.46      0.50      0.48      1101\n",
      "weighted avg       0.86      0.93      0.89      1101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train a linear SVM.\n",
    "# Regularized linear models with stochastic gradient descent (SGD) learning.\n",
    "svm_tuned = SGDClassifier(loss = \"hinge\", \n",
    "                    penalty = \"l2\",\n",
    "                    alpha = 1e-1, \n",
    "                    random_state = 42,\n",
    "                    max_iter = 5, \n",
    "                    tol = None)\n",
    "svm_tuned = svm_tuned.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on test set.\n",
    "y_svm_tuned = svm_tuned.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate performance.\n",
    "print(\"\\n--- LINEAR SUPPORT VECTOR MACHINE ---\\n\")\n",
    "print(metrics.classification_report(y_test, y_svm_tuned))\n",
    "\n",
    "# SVM performance metrics.\n",
    "# 0 = HAM, 1 = SPAM.\n",
    "# Care most about false negatives – keeping SPAM when we only want HAM.\n",
    "# Recall, F1, and accuracy rely on false negatives.\n",
    "confusion_svm_tuned = metrics.confusion_matrix(y_test, y_svm_tuned)\n",
    "acc_svm_tuned = metrics.accuracy_score(y_test, y_svm_tuned)\n",
    "f1_svm_tuned = metrics.f1_score(y_test, y_svm_tuned, zero_division = 0)\n",
    "precision_svm_tuned = metrics.precision_score(y_test, y_svm_tuned, zero_division = 0)\n",
    "recall_svm_tuned = metrics.recall_score(y_test, y_svm_tuned, zero_division = 0)\n",
    "roc_svm_tuned = metrics.roc_auc_score(y_test, y_svm_tuned)\n",
    "        \n",
    "print(\"\\n--- LINEAR SUPPORT VECTOR MACHINE ---\\n\")\n",
    "print(\"\\n---------------------------------------------\")\n",
    "print(\"tn, fp, fn, tp =\", confusion_svm_tuned.ravel())\n",
    "print(\"F1             =\", f1_svm_tuned)\n",
    "print(\"Accuracy       =\", acc_svm_tuned)\n",
    "print(\"Precision      =\", precision_svm_tuned)\n",
    "print(\"Recall         =\", recall_svm_tuned)\n",
    "print(\"ROC AUC        =\", roc_svm_tuned)\n",
    "print(\"---------------------------------------------\\n\")\n",
    "\n",
    "# Evaluate performance.\n",
    "print(\"\\n--- CLASSIFICATION REPORT ---\\n\")\n",
    "print(metrics.classification_report(y_test, y_svm_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d45bd90c-f6c8-482a-b0d9-710044f80978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LINEAR SUPPORT VECTOR MACHINE ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95      1020\n",
      "           1       0.31      0.14      0.19        81\n",
      "\n",
      "    accuracy                           0.91      1101\n",
      "   macro avg       0.62      0.56      0.57      1101\n",
      "weighted avg       0.89      0.91      0.90      1101\n",
      "\n",
      "\n",
      "--- LINEAR SUPPORT VECTOR MACHINE ---\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "tn, fp, fn, tp = [995  25  70  11]\n",
      "F1             = 0.18803418803418803\n",
      "Accuracy       = 0.9137148047229791\n",
      "Precision      = 0.3055555555555556\n",
      "Recall         = 0.13580246913580246\n",
      "ROC AUC        = 0.5556463326071168\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "--- CLASSIFICATION REPORT ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95      1020\n",
      "           1       0.31      0.14      0.19        81\n",
      "\n",
      "    accuracy                           0.91      1101\n",
      "   macro avg       0.62      0.56      0.57      1101\n",
      "weighted avg       0.89      0.91      0.90      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a linear SVM.\n",
    "# Regularized linear models with stochastic gradient descent (SGD) learning.\n",
    "svm_tuned = SGDClassifier(loss = \"hinge\", \n",
    "                    penalty = \"l2\",\n",
    "                    alpha = 1e-4, \n",
    "                    random_state = 42,\n",
    "                    max_iter = 5, \n",
    "                    tol = None)\n",
    "svm_tuned = svm_tuned.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on test set.\n",
    "y_svm_tuned = svm_tuned.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate performance.\n",
    "print(\"\\n--- LINEAR SUPPORT VECTOR MACHINE ---\\n\")\n",
    "print(metrics.classification_report(y_test, y_svm_tuned))\n",
    "\n",
    "# SVM performance metrics.\n",
    "# 0 = HAM, 1 = SPAM.\n",
    "# Care most about false negatives – keeping SPAM when we only want HAM.\n",
    "# Recall, F1, and accuracy rely on false negatives.\n",
    "confusion_svm_tuned = metrics.confusion_matrix(y_test, y_svm_tuned)\n",
    "acc_svm_tuned = metrics.accuracy_score(y_test, y_svm_tuned)\n",
    "f1_svm_tuned = metrics.f1_score(y_test, y_svm_tuned, zero_division = 0)\n",
    "precision_svm_tuned = metrics.precision_score(y_test, y_svm_tuned, zero_division = 0)\n",
    "recall_svm_tuned = metrics.recall_score(y_test, y_svm_tuned, zero_division = 0)\n",
    "roc_svm_tuned = metrics.roc_auc_score(y_test, y_svm_tuned)\n",
    "        \n",
    "print(\"\\n--- LINEAR SUPPORT VECTOR MACHINE ---\\n\")\n",
    "print(\"\\n---------------------------------------------\")\n",
    "print(\"tn, fp, fn, tp =\", confusion_svm_tuned.ravel())\n",
    "print(\"F1             =\", f1_svm_tuned)\n",
    "print(\"Accuracy       =\", acc_svm_tuned)\n",
    "print(\"Precision      =\", precision_svm_tuned)\n",
    "print(\"Recall         =\", recall_svm_tuned)\n",
    "print(\"ROC AUC        =\", roc_svm_tuned)\n",
    "print(\"---------------------------------------------\\n\")\n",
    "\n",
    "# Evaluate performance.\n",
    "print(\"\\n--- CLASSIFICATION REPORT ---\\n\")\n",
    "print(metrics.classification_report(y_test, y_svm_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9178b-93fd-4766-bf47-616041c4b215",
   "metadata": {},
   "source": [
    "## Optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aabe09b8-1d51-4b5a-a706-bffbf4ac4636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 1034\n",
      "\n",
      "--- LINEAR SUPPORT VECTOR MACHINE ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95      1020\n",
      "           1       0.31      0.14      0.19        81\n",
      "\n",
      "    accuracy                           0.91      1101\n",
      "   macro avg       0.62      0.56      0.57      1101\n",
      "weighted avg       0.89      0.91      0.90      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BEST PERFORMING MODEL = linear SVM with below hyperparameters.\n",
    "# Regularized linear models with stochastic gradient descent (SGD) learning.\n",
    "svm_opt = SGDClassifier(loss = \"hinge\", \n",
    "                        penalty = \"l2\",\n",
    "                        alpha = 1e-4, \n",
    "                        random_state = 42,\n",
    "                        max_iter = 5, \n",
    "                        tol = None)\n",
    "\n",
    "svm_opt = svm_opt.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on test set.\n",
    "y_svm_opt = svm_opt.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate performance.\n",
    "print(\"\\n--- LINEAR SUPPORT VECTOR MACHINE ---\\n\")\n",
    "print(metrics.classification_report(y_test, y_svm_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe03990-e94f-40b7-8646-0ed3c40687de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b76442-a21b-4d81-98a2-69ab3c3ba0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
